experiment_name: "RL_test"
environment:
  class_path: atcenv.src.environment_objects.environment.DefaultEnvironment
  init_args:
    render_frequency: 0
model:
  class_path: atcenv.src.models.sac.SAC
  init_args:
    action_dim: 2
    buffer:
      class_path: atcenv.src.models.replay_buffer.ReplayBuffer
      init_args:
        obs_dim: 10
        action_dim: 2
        size: 1000000
    actor:
      class_path: atcenv.src.models.actor.FeedForwardActor
      init_args:
        in_dim: 10
        out_dim: 2
    critic_q_1:
      class_path: atcenv.src.models.critic_q.FeedForward_Q
      init_args:
        in_dim: 12
    critic_q_2:
      class_path: atcenv.src.models.critic_q.FeedForward_Q
      init_args:
        in_dim: 12
    critic_v:
      class_path: atcenv.src.models.critic_v.FeedForward_V
      init_args:
        in_dim: 10
    critic_v_target:
      class_path: atcenv.src.models.critic_v.FeedForward_V
      init_args:
        in_dim: 10
scenario:
  num_episodes: 20000
  num_flights: 10
  airspace_area: 50000000000
  traffic_density: 0
  test_scenario_dir: "hello"
  num_test_episodes: 0
  test_frequency: 0
airspace:
  class_path: atcenv.src.environment_objects.airspace.EnrouteAirspace
aircraft:
  min_speed: 200
  max_speed: 250
  min_distance: 10000
observation:
  class_path: atcenv.src.observation.observation.Local
  init_args:
    observation_size: 10
    num_ac_state: 1
    normalize_data: True
    create_normalization_data: False
    normalization_data_file: "local_normalization.p"
    normalization_samples: 500000
reward:
  class_path: atcenv.src.reward.reward.DefaultReward
logger:
  class_path: atcenv.src.logger.logger.BasicLogger
  init_args:
    log_frequency: 0
    verbose: True
